<div align="center">

# CBraMod


_A Criss-Cross Brain Foundation Model for EEG Decoding_


[![Paper](https://img.shields.io/badge/arXiv-2412.07236-red)](https://arxiv.org/abs/2412.07236)
[![Paper](https://img.shields.io/badge/Paper-ICLR-008B8B)](https://openreview.net/forum?id=NPNUHgHF2w)
[![huggingface](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-FFD21E)](https://huggingface.co/weighting666/CBraMod)
![GitHub Repo stars](https://img.shields.io/github/stars/wjq-learning/CBraMod)

</div>


<div align="center">
<img src="figure/CBraMod_logo.png" style="width: 15%;" />
</div>


<p align="center">
    ğŸ”&nbsp;<a href="#-about">About</a>
    | ğŸ”¨&nbsp;<a href="#-setup">Setup</a>
    | ğŸš¢&nbsp;<a href="#-how-to-pretrain">How to Pretrain</a>
    | â›µ&nbsp;<a href="#-how-to-finetune">How to Finetune</a>
    | ğŸš€&nbsp;<a href="#-quick-start">Quick Start</a>
    | ğŸ”—&nbsp;<a href="#-citation">Citation</a>
</p>

ğŸ”¥ NEWS: The paper "_CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding_" has been accepted by ICLR 2025!

## ğŸ” About
We propose **CBraMod**, a novel EEG foundation model, for EEG decoding on various clinical and BCI application.
The preprint version of our paper is available at [arXiv](https://arxiv.org/abs/2412.07236). 
The camera-ready version of the paper will be available at [OpenReview](https://openreview.net/forum?id=NPNUHgHF2w).
<div align="center">
<img src="figure/model.png" style="width:100%;" />
</div>



## ğŸ”¨ Setup
Install [Python](https://www.python.org/downloads/).

Install [PyTorch](https://pytorch.org/get-started/locally/).

Install other requirements:
```commandline
pip install -r requirements.txt
``` 


## ğŸš¢ How to Pretrain
You can pretrain CBraMod on our pretraining dataset or your custom pretraining dataset using the following code:
```commandline
python cbramod/training/pretraining/pretrain_main.py
```
We have released a pretrained checkpoint on [HugginfaceğŸ¤—](https://huggingface.co/weighting666/CBraMod).

## â›µ How to Finetune

### Standard Fine-tuning
```commandline
# 4-class training with v1 mapping (recommended)
python cbramod/training/finetuning/finetune_main.py \
    --downstream_dataset IDUN_EEG \
    --datasets_dir data/datasets/final_dataset \
    --datasets ORP,2023_Open_N,2019_Open_N,2017_Open_N \
    --use_pretrained_weights True \
    --model_dir "./saved_models" \
    --tune \
    --num_of_classes 4 \
    --multi_eval \
    --label_mapping_version v1
```

### Two-Phase Training (Advanced)
```commandline
python cbramod/training/finetuning/finetune_main.py \
    --two_phase_training True \
    --epochs 15 \
    --num_of_classes 4
```

### Inference
```commandline
python scripts/inference/Inference_local.py
```


## ğŸš€ Quick Start
You can fine-tune the pretrained CBraMod on your custom downstream dataset using the following example code:
```python
import torch
import torch.nn as nn
from models.cbramod import CBraMod
from einops.layers.torch import Rearrange

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = CBraMod().to(device)
model.load_state_dict(torch.load('saved_models/pretrained/pretrained_weights.pth', map_location=device))
model.proj_out = nn.Identity()
classifier = nn.Sequential(
  Rearrange('b c s p -> b (c s p)'),
  nn.Linear(22*4*200, 4*200),
  nn.ELU(),
  nn.Dropout(0.1),
  nn.Linear(4 * 200, 200),
  nn.ELU(),
  nn.Dropout(0.1),
  nn.Linear(200, 4),
).to(device)

# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)
mock_eeg = torch.randn((8, 22, 4, 200)).to(device)

# logits.shape = (batch_size, num_of_classes)
logits = classifier(model(mock_eeg))
```

## ğŸ“ Directory Structure

The codebase has been reorganized for clarity and maintainability:

```
CBraMod/
â”œâ”€â”€ cbramod/                    # Core module (main implementation)
â”‚   â”œâ”€â”€ models/                 # Model architectures (cbramod.py, criss_cross_transformer.py)
â”‚   â”œâ”€â”€ load_datasets/          # Dataset loaders (idun_datasets.py, enhanced_dataset.py)
â”‚   â”œâ”€â”€ preprocessing/          # EEG preprocessing pipelines
â”‚   â”œâ”€â”€ training/               # Training scripts (finetuning/, pretraining/)
â”‚   â””â”€â”€ utils/                  # Utilities (signaltools.py, memory_manager.py)
â”œâ”€â”€ saved_models/               # Consolidated model storage
â”‚   â”œâ”€â”€ pretrained/            # Foundation model weights
â”‚   â”œâ”€â”€ finetuned/             # Best performing fine-tuned models
â”‚   â””â”€â”€ production/            # Production-ready models
â”œâ”€â”€ experiments/               # Experiment tracking and results
â”‚   â”œâ”€â”€ logs/                  # Training logs
â”‚   â”œâ”€â”€ configs/               # Reproducibility configurations
â”‚   â””â”€â”€ results/               # Analysis results and figures
â”œâ”€â”€ data/                      # EEG datasets
â”‚   â””â”€â”€ datasets/              # Processed dataset files
â”œâ”€â”€ deploy_prod/               # Production deployment code
â”œâ”€â”€ scripts/                   # Utility scripts (inference, setup)
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ Plot/                      # Research analysis plots
â””â”€â”€ Plot_Clean/                # Publication-ready plots
```

### Key Features
- **Clean Structure**: Consolidated model storage and experiment tracking
- **No Duplicates**: Removed scattered weights and logs across multiple directories  
- **Easy Navigation**: Clear separation of concerns with logical grouping
- **Production Ready**: Dedicated deployment and production model directories

## ğŸ”— Citation
If you're using this repository in your research or applications, please cite using the following BibTeX:
```bibtex
@inproceedings{wang2025cbramod,
    title={{CB}raMod: A Criss-Cross Brain Foundation Model for {EEG} Decoding},
    author={Jiquan Wang and Sha Zhao and Zhiling Luo and Yangxuan Zhou and Haiteng Jiang and Shijian Li and Tao Li and Gang Pan},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=NPNUHgHF2w}
}
```

## â­ Star History
<div align="center">
    <a href="https://star-history.com/#wjq-learning/CBraMod&Date">
        <img src="https://api.star-history.com/svg?repos=wjq-learning/CBraMod&type=Date" style="width: 80%;" />
    </a>
</div>